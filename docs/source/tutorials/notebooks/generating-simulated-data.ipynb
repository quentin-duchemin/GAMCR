{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulated data has been generated using the threebox model from [Kirchner 2019](https://hess.copernicus.org/articles/23/303/2019/). \n",
    "\n",
    "\n",
    "In this notebook, we explain the procedure we followed to generate the 6 benchmark datasets used in our paper.\n",
    "\n",
    "## /!\\ The Pyeto Python package should be downloaded from [here](https://github.com/woodcrafty/PyETo) and the path of its location on your laptop should be added using the command below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('{Path where you saved the Pyeto package}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading the precipitation time series and computing potential evapotranspiration using daily (Tmax, Tmin and Tmean)\n",
    "\n",
    "\n",
    "Note that you will need the (hourly) precipitation time series and the daily maximum, minimum and mean temperature at the site of interest. This data can be computed for example on the MeteoSwiss' API for catchment located in Switzerland."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import pyeto\n",
    "import GAMCR\n",
    "\n",
    "def toYearFraction(date):\n",
    "    \"\"\"\n",
    "    Allows to convert a date into a timeyear object (i.e. such as 2022.5 for summer in 2022).\n",
    "    \"\"\"\n",
    "    def sinceEpoch(date): # returns seconds since epoch\n",
    "        return (date.timestamp())\n",
    "    s = sinceEpoch\n",
    "\n",
    "    year = date.year\n",
    "    startOfThisYear = datetime(year=year, month=1, day=1)\n",
    "    startOfNextYear = datetime(year=year+1, month=1, day=1)\n",
    "\n",
    "    yearElapsed = s(date) - s(startOfThisYear)\n",
    "    yearDuration = s(startOfNextYear) - s(startOfThisYear)\n",
    "    fraction = yearElapsed/yearDuration\n",
    "\n",
    "    return date.year + fraction\n",
    "\n",
    "# Defining the site and its latitude\n",
    "site = 'BAS'\n",
    "site2name = {'SIO':'Sion', 'LUG':'Lugano', 'PUY':'Pully', 'BAS':'Basel'}\n",
    "ls_latitude = {'LUG': 46, 'SIO': 46.13, 'PUY': 46.31, 'BAS':47.32}\n",
    "\n",
    "\n",
    "# Loading the precipitation data\n",
    "df = pd.read_csv('./precipitation/order_122969_{0}_rre150h0_1_data.txt'.format(site), sep=';')\n",
    "df['datetime'] = df['time'].apply(lambda x: datetime.strptime(str(x), '%Y%m%d%H'))\n",
    "df['t'] = df['datetime'].apply(lambda x: toYearFraction(x))\n",
    "\n",
    "df = df.replace('-', np.NaN)\n",
    "df = df.rename(columns={\"rre150h0\": \"p\"})\n",
    "df['p'] = df['p'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making sure there is no gaps in the data\n",
    "diffhour = (df['datetime'].sort_values().diff() > pd.to_timedelta('1 hour')).to_numpy()\n",
    "idxs = np.where(diffhour>0.5)[0]\n",
    "plt.plot(diffhour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading daily temprature data\n",
    "Tmax = pd.read_csv('./daily_temperature_min_max/order_123003_{0}_tre200dx_1_data.txt'.format(site), header=0, sep=';', engine='python')\n",
    "Tmax['datetime'] = Tmax['time'].apply(lambda x: datetime.strptime(str(x), '%Y%m%d'))\n",
    "Tmax['t'] = Tmax['datetime'].apply(lambda x: toYearFraction(x))\n",
    "Tmax = Tmax.rename(columns={\"tre200dx\": \"tmax\"})\n",
    "Tmax = Tmax.replace('-', np.NaN)\n",
    "\n",
    "Tmin = pd.read_csv('./daily_temperature_min_max/order_123003_{0}_tre200dn_1_data.txt'.format(site), header=0, sep=';',  engine='python')\n",
    "Tmin['datetime'] = Tmin['time'].apply(lambda x: datetime.strptime(str(x), '%Y%m%d'))\n",
    "Tmin['t'] = Tmin['datetime'].apply(lambda x: toYearFraction(x))\n",
    "Tmin = Tmin.rename(columns={\"tre200dn\": \"tmin\"})\n",
    "Tmin = Tmin.replace('-', np.NaN)\n",
    "\n",
    "Tmean = pd.read_csv('./daily_temperature_mean/order_122970_{0}_tre200d0_1_data.txt'.format(site), header=0, sep=';',  engine='python')\n",
    "Tmean['datetime'] = Tmean['time'].apply(lambda x: datetime.strptime(str(x), '%Y%m%d'))\n",
    "Tmean['t'] = Tmean['datetime'].apply(lambda x: toYearFraction(x))\n",
    "Tmean = Tmean.rename(columns={\"tre200d0\": \"tabs\"})\n",
    "Tmean = Tmean.replace('-', np.NaN)\n",
    "\n",
    "data = Tmin\n",
    "#data = pd.merge(data, Tmin, on=\"datetime\", how=\"inner\")\n",
    "data = pd.merge(data, Tmax, on=\"datetime\", how=\"inner\")\n",
    "data = pd.merge(data, Tmean, on=\"datetime\", how=\"inner\")\n",
    "\n",
    "diffhour = (data['datetime'].sort_values().diff() > pd.to_timedelta('1 day')).to_numpy()\n",
    "idxs = np.where(diffhour>0.5)[0]\n",
    "plt.plot(diffhour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing PET using the Hargreaves method implemented in the Pyeto Package\n",
    "def get_PET_hargreaves(tmin,tmean,tmax,Date,latitude):\n",
    "    lat = pyeto.deg2rad(float(latitude))  # Convert latitude to radians\n",
    "    day_of_year = Date.timetuple().tm_yday\n",
    "    sol_dec = pyeto.sol_dec(day_of_year)            # Solar declination\n",
    "    sha = pyeto.sunset_hour_angle(lat, sol_dec)\n",
    "    ird = pyeto.inv_rel_dist_earth_sun(day_of_year)\n",
    "    et_rad = pyeto.et_rad(lat, sol_dec, sha, ird)   # Extraterrestrial radiation\n",
    "    tmax = max(tmax,tmin)\n",
    "    tmin = min(tmin,tmax)\n",
    "    tmean = max(min(tmean,tmax),tmin)\n",
    "    return pyeto.hargreaves(tmin, tmax, tmean, et_rad)\n",
    "\n",
    "lst = np.arange(0,len(data),1)\n",
    "PET = np.array(list(map(lambda t: get_PET_hargreaves(data.iloc[t]['tmin'], data.iloc[t]['tabs'], data.iloc[t]['tmax'], data['datetime'][t], ls_latitude[site]), lst) ))\n",
    "data['pet'] = PET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We add the PET data to the dataframe with precipitation making sure that dates match\n",
    "df = df.sort_values(by='datetime', axis=0)\n",
    "data = data.sort_values(by='datetime', axis=0)\n",
    "begindate = max(df.iloc[0]['datetime'], data.iloc[0]['datetime'])\n",
    "enddate = min(df.iloc[-1]['datetime'], data.iloc[-1]['datetime'])\n",
    "enddate = enddate-pd.to_timedelta('1 hour')\n",
    "print(begindate, enddate)\n",
    "df = df.loc[df['datetime'].apply(lambda x: (begindate<=x) and (enddate>=x))]\n",
    "data = data.loc[data['datetime'].apply(lambda x: (begindate<=x) and (enddate>=x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We convert daily PET to hourly data\n",
    "df_index = data[['datetime','pet']].set_index('datetime')\n",
    "hours2daily = df['datetime'].apply(lambda x: df_index.loc[x.strftime('%Y-%m-%d')].values[0])\n",
    "#hours2hourlypet = [data.loc[data['datetime']==day]['pet'].values[0]/24 for day in hours2days]\n",
    "df['pet'] = hours2daily/24\n",
    "\n",
    "df.fillna(0, inplace=True)\n",
    "df['p'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={\"t\": \"timeyear\"})\n",
    "df['date'] = df['timeyear'].apply(lambda x: GAMCR.fractional_year_to_datetime(x))\n",
    "\n",
    "# We save the dataframe\n",
    "df.to_csv('data_{0}.csv'.format(site2name[site]), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. (Optional) Calibrating the parameters of the threebox model\n",
    "\n",
    "When considering precipitation data from a real site, one might want to generate streamflow values that are realistic for this site. To do so, one can calibrate the model parameters of the threebox models using both precipitation and streamflow time series at the given site (or at a similar site for which data is available)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Running the threebox with to get the simulated streamflow time series\n",
    "\n",
    "You have two possible options here:\n",
    "\n",
    "- either you simply want to get the streamflow time series:\n",
    "In this case you can use the Rscript `generate_streamflow.R` located in threeboxmodel/Rscripts.\n",
    "- or you also want to compute the true transfer functions:\n",
    "In this case you can use the Rscript `simulations_computing_tfs_flashy.R` located in threeboxmodel/Rscripts. This script will save the data related to the simulation on the complete precipitation time series in a file entitled `ref_flow_notflashy_fluxes.txt`. The Rscript then launched the simulator again by removing a single precipitation event. Note that you can specify the minimum precipitation intensity for which you want to compute the corresponding transfer function. By default, the threshold is set to 1 (mm/h). \n",
    "Once the Rscript is finished, you can use the python script `save_tfs.py` (also located in threeboxmodel/Rscripts) to aggregate and save the transfer functions of every precipitation events of interest in a single file.\n",
    "\n",
    "Note that in both cases, you should modify the Rscript to: \n",
    "- change the path the file according to the location of the code on your laptop\n",
    "- change if needed the model parameters\n",
    "- change the name of the site considered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Visualizing the data\n",
    "\n",
    "You can visualize the simulated data (and the computed transfer functions) by relying on the notebook `visualization_data.ipynb` located in the folder `notebooks`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Making sure we have the right folder structure\n",
    "\n",
    "To properly use the GAMCR package, you should have a folder for each site with the following structure:\n",
    "\n",
    "- this folder should have name `site`\n",
    "- in this folder, you should have the `data_{site}.txt` file saved\n",
    "- GAMCR will save in this folder the different models that you will train for that site\n",
    "- in this folder, two subfolders will be created and used by GAMCR. \n",
    "    * The first subfolder `data` will be created to save the preprocessed data when calling a `save_batch` type method\n",
    "    * The second subfolder `results` will be created to save some statistics on the results of a trained model when calling the `compute_statistics` method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Screen shot of an example of simulated data.](../../_static/simulated_data.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
